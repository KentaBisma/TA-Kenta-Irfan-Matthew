{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Init"
      ],
      "metadata": {
        "id": "eJJpjjv5JaeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic"
      ],
      "metadata": {
        "id": "zcEKCRhKPACe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97388562-33bc-4811-860b-4961bc9f8016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bertopic\n",
            "  Downloading bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.22.4)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic)\n",
            "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn>=0.5.0 (from bertopic)\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.65.0)\n",
            "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.13.1)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (0.29.34)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2022.7.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.15.2+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (23.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (16.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp310-cp310-linux_x86_64.whl size=3541989 sha256=7e0e036ae85706cc1fa4630d0d8a83c03562d4cb81100d6fa2d18ab12391b5da\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/52/e3/6c6b60b126b4d5c4370cb5ac071b82950f91649d62d72f7f56\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=d4706f0ebeb881df85c7fc9bf73aea008948c5201dd4de3f02c0ab62abd00680\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82816 sha256=94251c566fe51afc2cd1227d5eb6cb78d05e25d961b7585e9c891db4c73372d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55622 sha256=265930ec46e82c60f842ac19ce059ef789aadc22498f06518f8535c624d7463c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "Successfully built hdbscan sentence-transformers umap-learn pynndescent\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\n",
            "Successfully installed bertopic-0.15.0 hdbscan-0.8.29 huggingface-hub-0.15.1 pynndescent-0.5.10 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.29.2 umap-learn-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from bertopic import BERTopic\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "from bertopic.dimensionality import BaseDimensionalityReduction\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from hdbscan import HDBSCAN\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "from statistics import mean\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from nltk.stem.porter import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from umap import UMAP\n",
        "import re\n",
        "import pprint\n",
        "np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Preprocess\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    stemmer = SnowballStemmer(language='english')\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(stemmer.stem(WordNetLemmatizer().lemmatize(token)))\n",
        "    return result\n",
        "\n",
        "# N gram maker\n",
        "def make_n_grams(texts, n, min_count=5, threshold=100):\n",
        "    n_gram = \"\"\n",
        "    n_gram_mod = \"\"\n",
        "    text_proc = texts\n",
        "    res = texts\n",
        "    for i in range(n-1):\n",
        "        n_gram = gensim.models.Phrases(text_proc, min_count=min_count, threshold=threshold)\n",
        "        n_gram_mod = gensim.models.phrases.Phraser(n_gram)\n",
        "        text_proc = n_gram[text_proc]\n",
        "        res = [n_gram[i] for i in res]\n",
        "    return res\n",
        "\n",
        "# Augment data manually\n",
        "def data_augment_manual(wordstocks, times):\n",
        "    for i in range(times):\n",
        "        wordstocks = wordstocks + wordstocks\n",
        "    return wordstocks\n",
        "\n",
        "# Document counter\n",
        "def data_augment(wordstocks):\n",
        "    result = wordstocks\n",
        "    data_len = len(wordstocks)\n",
        "    counter = 0\n",
        "    while True:\n",
        "        try:\n",
        "            topic_model_aut = BERTopic(nr_topics='auto')\n",
        "            topics_aut, probs_aut = topic_model_aut.fit_transform(result)\n",
        "            break\n",
        "        except:\n",
        "            result = result + result\n",
        "            counter += 1\n",
        "            continue\n",
        "    return (wordstocks, data_len, counter)\n",
        "\n",
        "# Topic modelling\n",
        "# Input : wordstocks, data length, number of topics\n",
        "# Output : topics and coherences for 5 models\n",
        "def topic_modelling(wordstocks, data_len, topic_num):\n",
        "    topicDict_aut = dict()\n",
        "    topicDict_ac = dict()\n",
        "    topicDict_km = dict()\n",
        "    topicDictLDA = dict()\n",
        "    topicDictLDAtrigram = dict()\n",
        "    # With HDBScan\n",
        "    topicBTAUT = None\n",
        "    coherence_aut = None\n",
        "    try:\n",
        "        topic_model_aut = BERTopic(nr_topics=topic_num)\n",
        "        topics_aut, probs_aut = topic_model_aut.fit_transform(wordstocks)\n",
        "        topicsAndName_aut = list(np.array(topic_model_aut.get_document_info(wordstocks)[[\"Topic\", \"Name\"]]))\n",
        "        for i in topicsAndName_aut:\n",
        "            topicDict_aut[i[0]] = re.sub(r'\\d+_', '', i[1]).replace(\"-\", \"\")\n",
        "        topicBTAUT = [topicDict_aut[i] for i in np.array(topic_model_aut.get_document_info(wordstocks)[\"Topic\"])][:data_len]\n",
        "        ### Compute Coherence Score of BERTopic Auto\n",
        "\n",
        "        documents_aut = pd.DataFrame({\"Document\": wordstocks,\n",
        "                              \"ID\": range(len(wordstocks)),\n",
        "                              \"Topic\": topics_aut})\n",
        "        dpt_aut = documents_aut.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
        "        cleaned_docs_aut = topic_model_aut._preprocess_text(dpt_aut.Document.values)\n",
        "\n",
        "        # Extract vectorizer and analyzer from BERTopic\n",
        "        vectorizer_aut = topic_model_aut.vectorizer_model\n",
        "        analyzer_aut = vectorizer_aut.build_analyzer()\n",
        "\n",
        "        # Extract features for Topic Coherence evaluation\n",
        "        words_aut = vectorizer_aut.vocabulary_.keys()\n",
        "        tokens_aut = [analyzer_aut(doc) for doc in cleaned_docs_aut]\n",
        "        dictionary_aut = gensim.corpora.Dictionary(tokens_aut)\n",
        "        corpus_aut = [dictionary_aut.doc2bow(token) for token in tokens_aut]\n",
        "        topic_words_aut = [[words for words, _ in topic_model_aut.get_topic(topic)] for topic in range(len(set(topics_aut))-1)]\n",
        "\n",
        "        # Evaluate\n",
        "        coherence_aut = CoherenceModel(topics=topic_words_aut,\n",
        "                                        texts=tokens_aut,\n",
        "                                        corpus=corpus_aut,\n",
        "                                        dictionary=dictionary_aut,\n",
        "                                        coherence='c_v').get_coherence()\n",
        "    except:\n",
        "        topicBTAUT = None\n",
        "        coherence_aut = None\n",
        "\n",
        "    # Agglomerative Cluster\n",
        "    topic_model_ac = BERTopic(hdbscan_model=AgglomerativeClustering(n_clusters=topic_num))\n",
        "    topics_ac, probs_ac = topic_model_ac.fit_transform(wordstocks)\n",
        "    topicsAndName_ac = list(np.array(topic_model_ac.get_document_info(wordstocks)[[\"Topic\", \"Name\"]]))\n",
        "    for i in topicsAndName_ac:\n",
        "        topicDict_ac[i[0]] = re.sub(r'\\d+_', '', i[1])\n",
        "    topicBTAC = [topicDict_ac[i] for i in np.array(topic_model_ac.get_document_info(wordstocks)[\"Topic\"])][:data_len]\n",
        "\n",
        "    # KMeans Clustering\n",
        "    topic_model_km = BERTopic(hdbscan_model=KMeans(n_clusters=topic_num))\n",
        "    topics_km, probs_km = topic_model_km.fit_transform(wordstocks)\n",
        "    topicsAndName_km = list(np.array(topic_model_km.get_document_info(wordstocks)[[\"Topic\", \"Name\"]]))\n",
        "    for i in topicsAndName_km:\n",
        "        topicDict_km[i[0]] = re.sub(r'\\d+_', '', i[1])\n",
        "    topicBTKM = [topicDict_km[i] for i in np.array(topic_model_km.get_document_info(wordstocks)[\"Topic\"])][:data_len]\n",
        "\n",
        "    # With LDA bigram\n",
        "    doc_lists = make_n_grams([i.split(\" \") for i in wordstocks], 2)\n",
        "    id2word = gensim.corpora.Dictionary(doc_lists)\n",
        "    corpus = [id2word.doc2bow(i) for i in doc_lists]\n",
        "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=topic_num, random_state=100, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
        "    topicDictLDA = {inner_list[0]: inner_list[1] for inner_list in [[topic_num, re.sub(r'\\d+\\.\\d+|\\d+', '', words.replace('\"', '').replace(\"*\", \"\").replace(' + ', '_'))] for topic_num, words in lda_model.print_topics(num_words=4)]}\n",
        "    ldaRes = []\n",
        "    for i in corpus:\n",
        "        max = ()\n",
        "        max_val = -99\n",
        "        for j,k in lda_model.get_document_topics(i):\n",
        "            if k > max_val:\n",
        "                max = (j,k)\n",
        "                max_val = k\n",
        "        ldaRes.append(topicDictLDA[max[0]])\n",
        "    topicLDA = ldaRes[:data_len]\n",
        "\n",
        "    # With LDA Trigram\n",
        "    doc_lists_trigram = make_n_grams([i.split(\" \") for i in wordstocks], 3)\n",
        "    id2word_trigram = gensim.corpora.Dictionary(doc_lists_trigram)\n",
        "    corpus_trigram = [id2word_trigram.doc2bow(i) for i in doc_lists_trigram]\n",
        "    lda_model_trigram = gensim.models.ldamodel.LdaModel(corpus=corpus_trigram, id2word=id2word_trigram, num_topics=topic_num, random_state=100, update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
        "    topicDictLDA_trigram = {inner_list[0]: inner_list[1] for inner_list in [[topic_num, re.sub(r'\\d+\\.\\d+|\\d+', '', words.replace('\"', '').replace(\"*\", \"\").replace(' + ', '_'))] for topic_num, words in lda_model_trigram.print_topics(num_words=4)]}\n",
        "    ldaRes_trigram = []\n",
        "    for i in corpus_trigram:\n",
        "        max = ()\n",
        "        max_val = -99\n",
        "        for j,k in lda_model_trigram.get_document_topics(i):\n",
        "            if k > max_val:\n",
        "                max = (j,k)\n",
        "                max_val = k\n",
        "        ldaRes_trigram.append(topicDictLDA_trigram[max[0]])\n",
        "    topicLDA_trigram = ldaRes_trigram[:data_len]\n",
        "\n",
        "    ### Compute Coherence Score of LDA bigram\n",
        "    coherence_lda = CoherenceModel(model=lda_model, texts=doc_lists, dictionary=id2word, coherence='c_v').get_coherence()\n",
        "\n",
        "    ### Compute Coherence Score of LDA trigram\n",
        "    coherence_lda_trigram = CoherenceModel(model=lda_model_trigram, texts=doc_lists_trigram, dictionary=id2word_trigram, coherence='c_v').get_coherence()\n",
        "\n",
        "\n",
        "    ### Compute Coherence Score of BERTopic Agglo\n",
        "\n",
        "\n",
        "    documents_ac = pd.DataFrame({\"Document\": wordstocks,\n",
        "                          \"ID\": range(len(wordstocks)),\n",
        "                          \"Topic\": topics_ac})\n",
        "    dpt_ac = documents_ac.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
        "    cleaned_docs_ac = topic_model_ac._preprocess_text(dpt_ac.Document.values)\n",
        "\n",
        "    # Extract vectorizer and analyzer from BERTopic\n",
        "    vectorizer_ac = topic_model_ac.vectorizer_model\n",
        "    analyzer_ac = vectorizer_ac.build_analyzer()\n",
        "\n",
        "    # Extract features for Topic Coherence evaluation\n",
        "    words_ac = vectorizer_ac.vocabulary_.keys()\n",
        "    tokens_ac = [analyzer_ac(doc) for doc in cleaned_docs_ac]\n",
        "    dictionary_ac = gensim.corpora.Dictionary(tokens_ac)\n",
        "    corpus_ac = [dictionary_ac.doc2bow(token) for token in tokens_ac]\n",
        "    topic_words_ac = [[words for words, _ in topic_model_ac.get_topic(topic)] for topic in range(len(set(topics_ac))-1)]\n",
        "\n",
        "    # Evaluate\n",
        "    coherence_ac = CoherenceModel(topics=topic_words_ac,\n",
        "                                    texts=tokens_ac,\n",
        "                                    corpus=corpus_ac,\n",
        "                                    dictionary=dictionary_ac,\n",
        "                                    coherence='c_v').get_coherence()\n",
        "\n",
        "    ### Compute Coherence Score of BERTopic KMeans\n",
        "\n",
        "\n",
        "    documents_km = pd.DataFrame({\"Document\": wordstocks,\n",
        "                          \"ID\": range(len(wordstocks)),\n",
        "                          \"Topic\": topics_km})\n",
        "    dpt_km = documents_km.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
        "    cleaned_docs_km = topic_model_km._preprocess_text(dpt_km.Document.values)\n",
        "\n",
        "    # Extract vectorizer and analyzer from BERTopic\n",
        "    vectorizer_km = topic_model_km.vectorizer_model\n",
        "    analyzer_km = vectorizer_km.build_analyzer()\n",
        "\n",
        "    # Extract features for Topic Coherence evaluation\n",
        "    words_km = vectorizer_km.vocabulary_.keys()\n",
        "    tokens_km = [analyzer_km(doc) for doc in cleaned_docs_km]\n",
        "    dictionary_km = gensim.corpora.Dictionary(tokens_km)\n",
        "    corpus_km = [dictionary_km.doc2bow(token) for token in tokens_km]\n",
        "    topic_words_km = [[words for words, _ in topic_model_km.get_topic(topic)] for topic in range(len(set(topics_km))-1)]\n",
        "\n",
        "    # Evaluate\n",
        "    coherence_km = CoherenceModel(topics=topic_words_km,\n",
        "                                    texts=tokens_km,\n",
        "                                    corpus=corpus_km,\n",
        "                                    dictionary=dictionary_km,\n",
        "                                    coherence='c_v').get_coherence()\n",
        "\n",
        "    return (topicLDA, topicLDA_trigram, topicBTAUT, topicBTAC, topicBTKM, coherence_lda, coherence_lda_trigram, coherence_aut, coherence_ac, coherence_km)\n",
        "\n",
        "# Topic modelling automatization for research purpose, this will run topic_modelling() for augmented datasets\n",
        "# Input : dataframe, topic numbers, and maximal number of augmentation\n",
        "# Output : list of dataframes containing (document, timestamp, and scores for each models), a dataframe of coherence scores, a concated list of both\n",
        "def auto_topic_modelling(df, topic_num=3, MAX_AUGMENT=4):\n",
        "    returned = []\n",
        "    coherence_np = []\n",
        "    list_of_both = []\n",
        "    dataset = np.array(df['text'])\n",
        "    timestamp = np.array(df['timestamp'])\n",
        "    wordstocks, data_len, counter = data_augment([\" \".join(preprocess(i)) for i in dataset])\n",
        "    start_time = datetime.now()\n",
        "    topicLDA, topicLDA_trigram, topicBTAUT, topicBTAC, topicBTKM, coherence_LDA, coherence_LDA_trigram, coherence_BTAUT, coherence_BTAC, coherence_BTKM = topic_modelling(wordstocks, data_len, topic_num)\n",
        "    if topicBTAUT is None:\n",
        "        csvTotal_aug = pd.DataFrame(data=[[dataset[i], timestamp[i], \"Error:Dataset too small\", topicBTAC[i], topicBTKM[i], topicLDA[i], topicLDA_trigram[i]] for i in range(data_len)], columns=[\"Document\", \"Timestamp\", \"BERTopic with HDBScan\", \"BERTopic with Agglomerative Topic\", \"BERTopic with KMeans\", \"LDA bigram\", \"LDA trigram\"])\n",
        "        end_time = datetime.now()\n",
        "        returned.append(csvTotal_aug)\n",
        "        coherence_np.append([coherence_LDA, coherence_LDA_trigram, \"Error:Dataset too small\", coherence_BTAC, coherence_BTKM, str('{} Second(s)'.format(end_time - start_time))])\n",
        "    else:\n",
        "        csvTotal_aug = pd.DataFrame(data=[[dataset[i], timestamp[i], topicBTAUT[i], topicBTAC[i], topicBTKM[i], topicLDA[i], topicLDA_trigram[i]] for i in range(data_len)], columns=[\"Document\", \"Timestamp\", \"BERTopic with HDBScan\", \"BERTopic with Agglomerative Topic\", \"BERTopic with KMeans\", \"LDA bigram\", \"LDA trigram\"])\n",
        "        end_time = datetime.now()\n",
        "        returned.append(csvTotal_aug)\n",
        "        coherence_np.append([coherence_LDA, coherence_LDA_trigram, coherence_BTAUT, coherence_BTAC, coherence_BTKM, str('{} Second(s)'.format(end_time - start_time))])\n",
        "        list_of_both.append([\"LDA bigram\", 0, coherence_LDA, [i for i in np.array(csvTotal_aug[['Document', 'LDA bigram', 'Timestamp']])]])\n",
        "        list_of_both.append([\"LDA trigram\", 0, coherence_LDA, [i for i in np.array(csvTotal_aug[['Document', 'LDA trigram', 'Timestamp']])]])\n",
        "        list_of_both.append([\"HDBScan\", 0, coherence_BTAUT, [i for i in np.array(csvTotal_aug[['Document', 'BERTopic with HDBScan', 'Timestamp']])]])\n",
        "        list_of_both.append([\"Agglomerative\", 0, coherence_BTAC, [i for i in np.array(csvTotal_aug[['Document', 'BERTopic with Agglomerative Topic', 'Timestamp']])]])\n",
        "        list_of_both.append([\"KMeans\", 0, coherence_BTKM, [i for i in np.array(csvTotal_aug[['Document', 'BERTopic with KMeans', 'Timestamp']])]])\n",
        "\n",
        "    for j in range(1, MAX_AUGMENT + 1):\n",
        "        wordstocks_temp = data_augment_manual(wordstocks, j)\n",
        "        start_time = datetime.now()\n",
        "        topicLDA, topicLDA_trigram, topicBTAUT, topicBTAC, topicBTKM, coherence_LDA, coherence_LDA_trigram, coherence_BTAUT, coherence_BTAC, coherence_BTKM = topic_modelling(wordstocks_temp, data_len, topic_num)\n",
        "        if topicBTAUT is None:\n",
        "            csvTotal_temp_aug = pd.DataFrame(data=[[dataset[i], timestamp[i], \"Error:Dataset too small\", topicBTAC[i], topicBTKM[i], topicLDA[i], topicLDA_trigram[i]] for i in range(data_len)], columns=[\"Document\", \"Timestamp\", \"BERTopic with HDBScan\", \"BERTopic with Agglomerative Topic\", \"BERTopic with KMeans\", \"LDA bigram\", \"LDA trigram\"])\n",
        "            end_time = datetime.now()\n",
        "            returned.append(csvTotal_temp_aug)\n",
        "            coherence_np.append([coherence_LDA, coherence_LDA_trigram, \"Error:Dataset too small\", coherence_BTAC, coherence_BTKM, str('{} Second(s)'.format(end_time - start_time))])\n",
        "        else:\n",
        "            csvTotal_temp_aug = pd.DataFrame(data=[[dataset[i], timestamp[i], topicBTAUT[i], topicBTAC[i], topicBTKM[i], topicLDA[i], topicLDA_trigram[i]] for i in range(data_len)], columns=[\"Document\", \"Timestamp\", \"BERTopic with HDBScan\", \"BERTopic with Agglomerative Topic\", \"BERTopic with KMeans\", \"LDA bigram\", \"LDA trigram\"])\n",
        "            end_time = datetime.now()\n",
        "            returned.append(csvTotal_temp_aug)\n",
        "            coherence_np.append([coherence_LDA, coherence_LDA_trigram, coherence_BTAUT, coherence_BTAC, coherence_BTKM, str('{} Second(s)'.format(end_time - start_time))])\n",
        "            list_of_both.append([\"LDA bigram\", j, coherence_LDA, [i for i in np.array(csvTotal_temp_aug[['Document', 'LDA bigram', 'Timestamp']])]])\n",
        "            list_of_both.append([\"LDA trigram\", j, coherence_LDA, [i for i in np.array(csvTotal_temp_aug[['Document', 'LDA trigram', 'Timestamp']])]])\n",
        "            list_of_both.append([\"HDBScan\", j, coherence_BTAUT, [i for i in np.array(csvTotal_temp_aug[['Document', 'BERTopic with HDBScan', 'Timestamp']])]])\n",
        "            list_of_both.append([\"Agglomerative\", j, coherence_BTAC, [i for i in np.array(csvTotal_temp_aug[['Document', 'BERTopic with Agglomerative Topic', 'Timestamp']])]])\n",
        "            list_of_both.append([\"KMeans\", j, coherence_BTKM, [i for i in np.array(csvTotal_temp_aug[['Document', 'BERTopic with KMeans', 'Timestamp']])]])\n",
        "\n",
        "    coherence_df = pd.DataFrame(data=coherence_np, columns=[\"coherence_LDA\", \"coherence_LDA_trigram\", \"coherence_BTAUT\", \"coherence_BTAC\", \"coherence_BTKM\", \"Time Taken\"])\n",
        "    return (returned, coherence_df, list_of_both)\n",
        "\n",
        "\n",
        "# Topic modelling automatization will be run for each topic numbers from 2 - 5 (for research purpose)\n",
        "# Input : dataframe, max topic numbers, and maximal number of augmentation\n",
        "# Output : list of lists of dataframes containing (document, timestamp, and scores for each models), a list of coherence scores, a concated list of lists of both\n",
        "def topic_model_auto_iter(df, MAX_AUGMENT = 4, MAX_TOPIC_NUM = 5):\n",
        "    list_of_lists_of_csvs = []\n",
        "    lists_of_coherences = []\n",
        "    list_of_lists_of_both = []\n",
        "    for i in range(2, MAX_TOPIC_NUM + 1):\n",
        "        res, coh, lob = auto_topic_modelling(df, i, MAX_AUGMENT)\n",
        "        list_of_lists_of_csvs.append(res)\n",
        "        lists_of_coherences.append(coh)\n",
        "        for j in lob:\n",
        "            list_of_lists_of_both.append([j[0], j[1], j[2], j[3], str(i)])\n",
        "    return (list_of_lists_of_csvs, lists_of_coherences, list_of_lists_of_both)\n",
        "\n",
        "\n",
        "# To find the highest coherence scores\n",
        "def run_tm(df, topic_num=5, MAX_AUGMENT=4):\n",
        "    res, coh, ttl = topic_model_auto_iter(df, MAX_AUGMENT, topic_num)\n",
        "    pairing = []\n",
        "    ttl_ind = max([x[2] for x in ttl])\n",
        "    # ttl_ind = max([x[1] for x in ttl])\n",
        "    for x in ttl:\n",
        "        if x[2] == ttl_ind:\n",
        "            pairing = x\n",
        "    return (pairing, [[x[0], x[1], x[2], x[4]] for x in ttl], [res, coh, ttl])\n",
        "\n",
        "# Saving the result\n",
        "def save(result, filename, col=[\"Document\", \"Timestamp\", \"Topics\"]):\n",
        "    # print(result)\n",
        "    pandas = pd.DataFrame(data=[[x[0], x[2], x[1]] for x in result[3]], columns=col)\n",
        "    pandas.to_csv(str(filename) + \"_Method_\" + str(result[0]) + \"_Augmentation_\" + str(result[1]) +\"_Topic_\" + str(result[4]) + \"_Score_\" + str(result[2]) + \"_\" + \".csv\", sep='\\t', index=False)\n",
        "    files.download(str(filename) + \"_Method_\" + str(result[0]) + \"_Augmentation_\" + str(result[1]) +\"_Topic_\" + str(result[4]) + \"_Score_\" + str(result[2]) + \"_\" + \".csv\")\n",
        "\n",
        "# Saving all possible results\n",
        "def save_all_coherence(results_list, filename, col=[\"Document\", \"Timestamp\", \"Topics\"]):\n",
        "    for i in results_list:\n",
        "        pandas = pd.DataFrame(data=[[x[0], x[2], x[1]] for x in i[3]], columns=col)\n",
        "        pandas.to_csv(str(filename) + \"_Method_\" + str(i[0]) + \"_Augmentation_\" + str(i[1]) +\"_Topic_\" + str(i[4]) + \"_Score_\" + str(i[2]) + \"_\" + \".csv\", sep='\\t', index=False)\n",
        "        files.download(str(filename) + \"_Method_\" + str(i[0]) + \"_Augmentation_\" + str(i[1]) +\"_Topic_\" + str(i[4]) + \"_Score_\" + str(i[2]) + \"_\" + \".csv\")\n",
        "\n",
        "# Run and save\n",
        "def run_save(df, filename, save_best_only = True, save_seer = True, topic_num=5, MAX_AUGMENT=4, col=[\"Document\", \"Timestamp\", \"Topics\"]):\n",
        "    pair, coh_lst, datas_file = run_tm(df, topic_num, MAX_AUGMENT)\n",
        "    ttl = datas_file[2]\n",
        "    if save_best_only:\n",
        "        save(pair, filename, col)\n",
        "    else:\n",
        "        for i in ttl:\n",
        "            save(i, filename, col)\n",
        "    if save_seer:\n",
        "        seer_df = seer(coh_lst)\n",
        "        seer_df.to_csv(str(filename) + \"_coherences_seer.csv\", sep='\\t', index=False)\n",
        "        files.download(str(filename) + \"_coherences_seer.csv\")\n",
        "\n",
        "# Getting the result for the dataset with the highest score\n",
        "def seer(coherence_lst):\n",
        "    df = pd.DataFrame(coherence_lst, columns=[\"Method\", \"Augmentation\", \"Score\", \"Topic Num\"])\n",
        "    grp_df = df.groupby([\"Topic Num\", \"Augmentation\"]).agg({\"Score\": list}).reset_index()\n",
        "    new_df = pd.DataFrame({\n",
        "        \"Topic Num\": grp_df[\"Topic Num\"],\n",
        "        \"Augmentation\": grp_df[\"Augmentation\"],\n",
        "        \"LDA bigram\": grp_df[\"Score\"].apply(lambda x: x[0]),\n",
        "        \"LDA trigram\": grp_df[\"Score\"].apply(lambda x: x[1]),\n",
        "        \"HDBScan\": grp_df[\"Score\"].apply(lambda x: x[2]),\n",
        "        \"Agglomerative\": grp_df[\"Score\"].apply(lambda x: x[3]),\n",
        "        \"KMeans\": grp_df[\"Score\"].apply(lambda x: x[4])\n",
        "    })\n",
        "    return new_df\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a41bdc13-aceb-461e-f875-138bb3b3eabe",
        "id": "kQUGPqyjhwvW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    }
  ]
}